{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc00a491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectado ao MongoDB Atlas com sucesso!\n",
      "Processando: AAPL\n",
      "Processando: MSFT\n",
      "Processando: AMZN\n",
      "Processando: GOOGL\n",
      "Processando: META\n",
      "Processando: NVDA\n",
      "Processando: TSLA\n",
      "Processando: JPM\n",
      "Processando: V\n",
      "Processando: PG\n",
      "Processando: KO\n",
      "Processando: PEP\n",
      "Processando: NFLX\n",
      "Processando: AMD\n",
      "Processando: INTC\n",
      "Processando: DIS\n",
      "Processando: PETR4.SA\n",
      "Processando: VALE3.SA\n",
      "Processando: ITUB4.SA\n",
      "Processando: BBDC4.SA\n",
      "Processando: ABEV3.SA\n",
      "Processando: WEGE3.SA\n",
      "Processando: BBAS3.SA\n",
      "Processando: B3SA3.SA\n",
      "Processando: RAIL3.SA\n",
      "Processando: PRIO3.SA\n",
      "Processando: LREN3.SA\n",
      "Processando: GGBR4.SA\n",
      "Atualizando 28 ativos no MongoDB...\n",
      "Universo de ações salvo no MongoDB Atlas.\n",
      "..//artifacts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rafae\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# train_supervised.py\n",
    "import os, json, time, datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from joblib import dump\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "connection_string = os.getenv(\"MONGODB_URI\")\n",
    "if not connection_string:\n",
    "    raise ValueError(\"MONGODB_URI não encontrada. Verifique seu arquivo .env\")\n",
    "\n",
    "client = MongoClient(connection_string)\n",
    "db = client[\"investia\"]\n",
    "assets_collection = db[\"assets\"] # Nome da sua coleção\n",
    "print(\"Conectado ao MongoDB Atlas com sucesso!\")\n",
    "\n",
    "BASE = \"../\"\n",
    "ART = f\"{BASE}/artifacts\"\n",
    "os.makedirs(ART, exist_ok=True)\n",
    "\n",
    "US_TICKERS = [\"AAPL\",\"MSFT\",\"AMZN\",\"GOOGL\",\"META\",\"NVDA\",\"TSLA\",\"JPM\",\"V\",\"PG\",\"KO\",\"PEP\",\"NFLX\",\"AMD\",\"INTC\",\"DIS\"]\n",
    "BR_TICKERS = [\"PETR4.SA\",\"VALE3.SA\",\"ITUB4.SA\",\"BBDC4.SA\",\"ABEV3.SA\",\"WEGE3.SA\",\"BBAS3.SA\",\"B3SA3.SA\",\"RAIL3.SA\",\"PRIO3.SA\",\"LREN3.SA\",\"GGBR4.SA\"]\n",
    "TICKERS = US_TICKERS + BR_TICKERS\n",
    "\n",
    "def region_for(t):\n",
    "    return \"BR\" if t.endswith(\".SA\") else \"US\"\n",
    "\n",
    "def _normalize_ohlcv(df):\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "    if hasattr(df, \"columns\") and isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = df.columns.get_level_values(-1)\n",
    "    cols = {c: c.strip() for c in df.columns}\n",
    "    df = df.rename(columns=cols)\n",
    "    if \"Close\" not in df.columns:\n",
    "        if \"Adj Close\" in df.columns:\n",
    "            df[\"Close\"] = df[\"Adj Close\"]\n",
    "        else:\n",
    "            return None\n",
    "    if \"Volume\" not in df.columns:\n",
    "        df[\"Volume\"] = np.nan\n",
    "    return df\n",
    "\n",
    "def fetch_single(ticker, start, end, interval=\"1d\", tries=3, sleep_s=0.6):\n",
    "    for _ in range(tries):\n",
    "        try:\n",
    "            t = yf.Ticker(ticker)\n",
    "            df = t.history(start=start, end=end, interval=interval, auto_adjust=False, actions=False, repair=True)\n",
    "            df = _normalize_ohlcv(df)\n",
    "            if df is not None and not df.empty:\n",
    "                idx = df.index\n",
    "                if hasattr(idx, \"tz\") and idx.tz is not None:\n",
    "                    df.index = idx.tz_convert(\"UTC\").tz_localize(None)\n",
    "                df = df.reset_index().rename(columns={\"Date\":\"date\"})\n",
    "                df[\"ticker\"] = ticker\n",
    "                return df\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(sleep_s)\n",
    "    try:\n",
    "        df = yf.download(ticker, period=\"1y\", interval=interval, auto_adjust=False, progress=False)\n",
    "        df = _normalize_ohlcv(df)\n",
    "        if df is not None and not df.empty:\n",
    "            idx = df.index\n",
    "            if hasattr(idx, \"tz\") and hasattr(idx, \"tz_localize\") and idx.tz is not None:\n",
    "                df.index = idx.tz_convert(\"UTC\").tz_localize(None)\n",
    "            df = df.reset_index().rename(columns={\"Date\":\"date\"})\n",
    "            df[\"ticker\"] = ticker\n",
    "            return df\n",
    "    except Exception:\n",
    "        pass\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def fetch_features(tickers, period_days=420, interval=\"1d\"):\n",
    "    end = dt.date.today()\n",
    "    start = end - dt.timedelta(days=period_days)\n",
    "    rows = []\n",
    "    \n",
    "    for t in tickers:\n",
    "        print(f\"Processando: {t}\")\n",
    "        \n",
    "        # --- PASSO 1: Buscar 'name' e 'setor' ---\n",
    "        try:\n",
    "            ticker_info = yf.Ticker(t).info\n",
    "            name = ticker_info.get('shortName', ticker_info.get('longName', t))\n",
    "            setor = ticker_info.get('sector', 'N/A')\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao buscar info de {t}: {e}. Usando valores padrão.\")\n",
    "            name = t\n",
    "            setor = \"N/A\"\n",
    "        \n",
    "        # --- PASSO 2: Buscar dados históricos ---\n",
    "        df = fetch_single(t, start=start, end=end, interval=interval)\n",
    "        \n",
    "        # --- PASSO 3: Checagens iniciais ---\n",
    "        if df is None or df.empty:\n",
    "            continue\n",
    "        if \"Close\" not in df.columns:\n",
    "            continue\n",
    "        df = df.dropna(subset=[\"Close\"])\n",
    "        if df.empty:\n",
    "            continue\n",
    "            \n",
    "        # --- PASSO 4: CALCULAR AS FEATURES (AQUI!) ---\n",
    "        # Este bloco estava faltando ou no lugar errado\n",
    "        df[\"ret_1d\"] = df[\"Close\"].pct_change()\n",
    "        df[\"ret_3m\"] = df[\"Close\"].pct_change(63)\n",
    "        df[\"ret_6m\"] = df[\"Close\"].pct_change(126)\n",
    "        df[\"vol_63\"] = df[\"ret_1d\"].rolling(63).std()\n",
    "        df[\"volavg_21\"] = df[\"Volume\"].rolling(21).mean()\n",
    "        \n",
    "        # --- PASSO 5: Filtrar para a última linha válida ---\n",
    "        # Agora o df tem as colunas novas\n",
    "        df = df.dropna().tail(1) \n",
    "        if df.empty:\n",
    "            # Isso pode acontecer se não houver dados suficientes\n",
    "            # para as janelas de 63 ou 126 dias\n",
    "            continue\n",
    "            \n",
    "        # --- PASSO 6: Adicionar à lista (AGORA É SEGURO) ---\n",
    "        # Agora df[\"ret_3m\"] com certeza existe\n",
    "        rows.append({\n",
    "            \"ticker\": t,\n",
    "            \"pais\": region_for(t),\n",
    "            \n",
    "            \"name\": name,\n",
    "            \"setor\": setor,\n",
    "            \n",
    "            \"ret_3m\": float(df[\"ret_3m\"].iloc[0]),\n",
    "            \"ret_6m\": float(df[\"ret_6m\"].iloc[0]),\n",
    "            \"vol_63\": float(df[\"vol_63\"].iloc[0]),\n",
    "            \"volavg_21\": float(df[\"volavg_21\"].iloc[0]),\n",
    "        })\n",
    "        time.sleep(0.1) \n",
    "    \n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def synth_labels(df):\n",
    "    v33 = df[\"vol_63\"].quantile(0.33)\n",
    "    v66 = df[\"vol_63\"].quantile(0.66)\n",
    "    profs = [\"conservador\",\"equilibrado\",\"ousado\"]\n",
    "    samples = []\n",
    "    for _, r in df.iterrows():\n",
    "        for p in profs:\n",
    "            label = 0\n",
    "            if p == \"conservador\" and r[\"vol_63\"] <= v33:\n",
    "                label = 1\n",
    "            if p == \"equilibrado\" and (r[\"vol_63\"] > v33 and r[\"vol_63\"] < v66):\n",
    "                label = 1\n",
    "            if p == \"ousado\" and r[\"vol_63\"] >= v66:\n",
    "                label = 1\n",
    "            samples.append({\n",
    "                \"ticker\": r[\"ticker\"],\n",
    "                \"pais\": r[\"pais\"],\n",
    "                \"ret_3m\": r[\"ret_3m\"],\n",
    "                \"ret_6m\": r[\"ret_6m\"],\n",
    "                \"vol_63\": r[\"vol_63\"],\n",
    "                \"volavg_21\": r[\"volavg_21\"],\n",
    "                \"perfil_conservador\": 1 if p==\"conservador\" else 0,\n",
    "                \"perfil_equilibrado\": 1 if p==\"equilibrado\" else 0,\n",
    "                \"perfil_ousado\": 1 if p==\"ousado\" else 0,\n",
    "                \"label\": label\n",
    "            })\n",
    "    return pd.DataFrame(samples)\n",
    "\n",
    "feats = fetch_features(TICKERS)\n",
    "if feats.empty:\n",
    "    raise RuntimeError(\"Sem dados de features. Verifique rede/yfinance.\")\n",
    "## feats.to_csv(f\"{ART}/universe_features.csv\", index=False)\n",
    "\n",
    "print(f\"Atualizando {len(feats)} ativos no MongoDB...\")\n",
    "data_to_save = feats.to_dict('records') \n",
    "\n",
    "for asset in data_to_save:\n",
    "    query = {\"ticker\": asset[\"ticker\"]} \n",
    "    update_data = {\"$set\": asset}     \n",
    "    assets_collection.update_one(query, update_data, upsert=True)\n",
    "\n",
    "print(\"Universo de ações salvo no MongoDB Atlas.\")\n",
    "\n",
    "data = synth_labels(feats)\n",
    "feature_cols = [\"ret_3m\",\"ret_6m\",\"vol_63\",\"volavg_21\",\"perfil_conservador\",\"perfil_equilibrado\",\"perfil_ousado\"]\n",
    "X = data[feature_cols].replace([np.inf,-np.inf], 0.0).fillna(0.0).values\n",
    "y = data[\"label\"].astype(int).values\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "Xtr, Xte, ytr, yte = train_test_split(Xs, y, test_size=0.2, random_state=42, stratify=y)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(16,8), activation=\"relu\", alpha=1e-4, max_iter=500, random_state=42)\n",
    "clf.fit(Xtr, ytr)\n",
    "dump(clf, f\"{ART}/reco_model.joblib\")\n",
    "dump(scaler, f\"{ART}/reco_scaler.joblib\")\n",
    "with open(f\"{ART}/feature_cols.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(feature_cols, f, ensure_ascii=False)\n",
    "print(ART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d4f195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
